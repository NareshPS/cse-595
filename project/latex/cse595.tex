%
% File acl2010.tex
%
% Contact  jshin@csie.ncnu.edu.tw or pkoehn@inf.ed.ac.uk
%%
%% Based on the style files for ACL-IJCNLP-2009, which were, in turn,
%% based on the style files for EACL-2009 and IJCNLP-2008...

%% Based on the style files for EACL 2006 by
%%e.agirre@ehu.es or Sergi.Balari@uab.es
%% and that of ACL 08 by Joakim Nivre and Noah Smith

\documentclass[12pt]{article}
\usepackage{cse595}
\usepackage{url}
\usepackage{latexsym}
\usepackage{graphicx}

%\setlength\titlebox{6.5cm}    % You can expand the title box if you
% really have to

\title{TagPandr: Tag exapansion for flickr images}

\author{Naresh Pratap Singh\\
  {\tt mail2naresh@gmail.com}  \And
  Rohith Menon\\
  {\tt rohithmenon@gmail.com} \And
  Sandesh Singh\\
  {\tt sandesh247@gmail.com}
  }

\date{}

\begin{document}
\maketitle
\begin{abstract}
We present an automatic method to suggest tags based on the existing tags provided by
users. We use co occurence statistics and wordnet to suggest tags in addition
to the existing tags, and use visual features to maintain the relevance of
the suggested tags to the image under consideration. We learn the visual features
for tags from Flickr image corpus. Experimental results show that better tags
  are suggested especially when the quality of original tags are low.
\end{abstract}

\section{Introduction}

A large number of photos are now available online, especially with photo sharing sites like
Flickr and Picassa. Annotation of images with relevant tags is a very useful feature especially
for image search, categorization and organization. While there has been advances in content
based analysis of images, scaling image data to web scale is a challenge.

In Flickr, tags are assigned by people who upload the photos. Tags irrelevant to the tasks of
image search and categorization happen to exist in the image tags. For eg: 2011 (the year) is generally
a tag associated with a photo taken on the new year of 2011. Similarly, model of camera is yet
another tag which is very specific to an image and not relevant to the content of it. Apart from
this, bulk upload of photo brings along with it the problem of bulk tagging. Especially for such
uploads, we find that a group of photos are tagged with the same set of tags. Such tagging, although
makes sense for the photo group as a whole, turns out to be irrelevant for many images in the group.
Poorly tagged images and under tagged images are other issues with commnunity tagging.

In this work, we present a method to automatically expand the given tags using text features and
rank the expanded tags with the visual features to highgly score those tags which are relevant to
the given image. We present multiple statistics about the suggested tags and also perform human
evaluation to compute the human perceived goodness of the suggested tags.

\section{Related Work}
Tag expansion and ranking has been a popular research topic in last few years. With the explosion of
images online on Flickr, Picasa, and Google Images etc., it became necessary to devise means of
organizing and managing images more effectively. Tags were introduced to solve this problem. Since,
they are mainly added by the users, they tend to represent contextual information about the image.
For instance, a cat in an image would be tagged by the name of the cat instead of the word \emph{cat}.
Significant research efforts are devoted for automatic annotation of tags \newcite{Liu:2009:TR:1526709.1526757}. 
The tags that describe the content of images can help
users easily manage and access large-scale image repositories.
With these metadata, the manipulations of image data
can be easier to be accomplished, such as browsing, indexing
and retrieval. Extensive research efforts have been dedicated
to automatically annotating images with a set of tags [6, 7,
8, 9]. These methods usually require a labeled training set
and then learn models with these data based on low-level features,
and then new unlabeled images can thus be annotated
using these models. Although a lot of encouraging results
have been reported, the performance of these approaches
are still far from satisfactory for practical large-scale application
due to the semantic gap between tags and low-level
features. Manual tagging that allows users to provide image
tags by themselves is an alternative approach. It of courses
adds users. labor costs in comparison with automatic tagging,
but it will provide more accurate tags. Manual tagging
has widely been adopted in image and video sharing websites
such as Flickr and Youtube, and the popularity of these
websites has demonstrated its rationality. However, as introduced
in Section 1, the user provided tags are orderless
and this significantly limits their applications. Asking users
to manually order the tags is obviously infeasible since it
will add much more labor costs for users. This difficulty
motivates our work. To the best of our knowledge, this is
the first work that aims to bring orders to image tags.

\section{Experimental Evaluation}
We performed experiments on flickr data set. Tags and images were separately downloaded using
flickr apis. We downloaded about 500,000 images from flickr belonging to categories: cat,
dog, train, aircraft, car, bus, places, crocodile, harley etc. We obtained 311,000 unique tags
corresponding to these images. Like mentioned before we filtered tags which contained numbers
and special characters. We found that such tags were not of much relevance to the images.

Evaluation of the tags would require some quantitative way of analysing quality. We evaluate
the tag quality automatically by two methods. Apart from these automatic methods, we also
perform human evaluation to score the tag quality.

\subsection{Original tag recall in top 5 suggested tags}
In this evaluation, we compute the number of tags in the original image tags that we suggest
in the top five tags automatically suggested by our method. The intuition behind this evaluation
is to make sure that the best tags we suggest have a good coverage of the original tags. Although
this method seems like a good measure of quality, the poor tags present in the original image
will cause this measure to degrade. Our method will fail to suggest original tags if there is
very less relevance with the given image. We calculate this measure of quality for tags generated
with text only features, visual only features and visual + text features. Table () tabulates the
experimental results.


\begin{table}
    \begin{tabular}{|l|l|l|l|}
aircraft & 0.92 & 0.46 & 0.61 \\
bus & 0.73 & 0.24 & 0.27 \\
car & 0.43 & 0.26 & 0.23 \\
cat & 0.71 & 0.21 & 0.23 \\
crocodile & 0.89 & 0.51 & 0.38 \\
dog & 0.76 & 0.21 & 0.30
    \end{tabular}
\end{table}

From the table its clear that text features alone have better overlap with the given tags.
This is because, the original tags will also be suggested as part of the text co-occurrence
model. We see a decline in the scores with visual only attributes and text+visual attributes.
We attribute this to the fact that for visual features, the ranking of tags comes purely from
the gist or spatial pyramid features and hence the overlap with the original tags will be lesser.
For text+visual features, the ranking of tags push better tags to the top, which has lower overlap
with the original tags compared to text only features.

\subsection{Prediction of original tags removed from test images}
For this evaluation, we randomly remove 30 percent of the orignal tags from the test images. We
then pass these images through our pipeline and compute the number of tags suggested from the
removed original tags in the top 20 of our suggested tags. The intuition behind this measure of
quality is to understand how well our models are able to simulate the quality of tags possessed
by original tags. This measure also has problems like mentioned before, but is a better measure
of quality because we look for the existence of the removed original tags in the suggested tag
set as a whole. We calculate this measure of quality for tags generated with text only features,
visual only features and visual + text only features. Table () tabulates the experimental results.


\begin{table}
    \begin{tabular}{|l|l|l|l|l|}
Category & Text & Visual & Text + Visual \\
aircraft & 0.87 & 1 & 0.70 \\
bus  & 0.33 & 0.67 & 0.29 \\
car &  0.34 & 0.87 & 0.27 \\
cat &  0.32 & 0.95 & 0.27 \\
crocodile &  0.36 & 1 & 0.34 \\
dog  & 0.29 & 0.89 & 0.22
    \end{tabular}
\end{table}

\subsection{Evaluation of classifier One Vs Rest}
For this experimental setup, we trained a classifier one for each unique tag appearing atleast
10 times in a particular category. We experimented the one vs rest classifier with support vector
machines to learn the visual features for individual tags. It turns out that the performance of
this classifier is worse when compared with a single classifier with multi-class prediction.

%% THIS IS HERE FOR LAYOUR PURPOSES oNLY

\begin{figure*}[h]
\includegraphics[width=0.8\textwidth]{tagExpansion.png}
\end{figure*}

\begin{figure*}[h]
\includegraphics[width=0.8\textwidth]{model.png}
\end{figure*}

We downloaded [a lot of] Flickr images, pertaining to pre-selected categories.

\subsection{Human Evaluation}
The above mentioned statistics would provide a notion of quality of the tags suggested. But
we cannot compare it with the quality of the original tags. For this reason, we perform human
evaluation of the input tags for the given image. Human evaluation is peformed for each of the
output tag and the original tags. A score from 1 to 4 is assigned to each of the tag, 1 being
lowest on relevance scale and 4 being highest on relevance scale. During evaluation, tags which
were about places and not obvious in the image as part of content were scored as 1. For meta images,
(ie) images which are photos of the writings, visual features will be of very little use. But
text features would prove to be good. Human evaluation was performed for tags expanded with
text only features, visual only features and text+visual features. We also performed human
evaluation of the original tags so that we could compare the scores of the original tags with
the suggested tags. Table() tabulates the results of the human evaluation.

Put the data.

We also found that when the number of tags in the image were less and those ones were very relevant,
the quality of tags expanded tended to be lesser. On the other hand when the original tags were bad,
we were able to suggest better quality tags. We studied the score pattern of original tags with respect
to the suggested tag. Figure() shows that when the original tag quality goes down, the suggested tag
quality goes up and vice versa.

\section{Implementation}
The image download was done on a hadoop cluster to reduce the amount of time
required to download all the images.
For each category, we performed a Google search to get canonical images for
those vcategories. We found that for any given keyword, the results returned
by Google were consistently more represnetative of that category. We used the
most similar Flickr images (based on Euclidean distance of the gist vectors)
for training.

We trained two models, a tag model and a visual model trained on the visual
features of the images. We filtered the tags such that any word containing
numbers was not considered for training the data. We claculated tag
cooccurence for every pair of tags in the training set, assuming independence
between the pairs. The probability of a given tag  a tag `tag2' depends
on the number of times they co occur with the prior tags. We build a bayes model to calculate the
probability of seeing a tag given a set of prior tags. We also extend the list
of tags using wordnet, where we choose synonyms to augment the original set.

We used two visual features, gist vectors and spatial pyramid based on
[someones] method. Gist was expected to help with global features such as
color and environment, while SIFT could help detect objects. We trained multiple
calssifiers on the visual features by using Weka's implementation of structure
learning of Bayesian networks using various hill climbing (K2, B, etc) and
general purpose (simulated annealing, tabu search) algorithms.


\section*{Acknowledgments}

Do not number the acknowledgment section. Do not include this section
when submitting your paper for review.
\bibliographystyle{cse595}
% you bib file should really go here
\bibliography{bib}

\end{document}
